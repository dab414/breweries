---
title: "Yelp Data Cleaning"
author: "Dave Braun"
date: "February 19, 2019"
output:
  html_document:
    df_print: paged
---


This document is intended for the cleaning of data scraped from yelp. The general layout of the data is as follows:  


```{r include = FALSE}
library(knitr)
library(tidyverse)
library(data.table)
library(chron)
library(imputeMissings)
library(fastDummies)
library(stringr)
```


```{r}
yelp <- read.csv('../data/yelpData.csv', stringsAsFactors = FALSE)
colnames(yelp) <- tolower(colnames(yelp))
## drop breweries that couldn't be scraped
yelp <-yelp[!is.na(yelp$ratings),]
head(yelp)
```

```{r}
yelp %>% 
  group_by(name) %>% 
  summarize(n()) %>% 
  nrow()
```

*Of the nearly 8,000 breweries on [open brewery db](https://openbrewerydb.org), I was able to successfully scrape 2465 of them on the first pass.*

Off the bat, I was a little worried because `rating` appears null across the board, but I see that it's redundant with `ratings`, which is present.  Some notes about specific vars:

* `moreInfoVar` The trickiest part will be handling `moreInfoVar`, as this capures idiosyncratic additional business detail, the features of which vary across businesses. It's represented in long form for now, but I'll want to experiment with getting it in wide so each brewery can be represented as a single observation.  

* `Hours` I'll probably want to parse the hours into total number of hours open each day. But this doesn't capture whether a brewery is open earlier / later. Maybe try to code for that with a different variable. Watch out for the `Open now` that's stored in the `Mon` hours. This will show up in the `Tues` hours in the data I'm collecting today.

* `claimed_status` Dunno what this is, seems to be constant across breweries so might as well drop.  

* `health_rating` I really like the idea of this one, but based on the first 400 breweries, this var is very sparse and heterogeneous.  

* `price_range` This one is important. Looks like it's represented with either an adjective (e.g., `moderate`) or price range (e.g., `$11-30`).  

* `ratings` `reviews` obv the dependent vars. I'll have to think about exactly how to negotiate the fact that there's two of them.  

* `ratings_histogram` I like the idea of capturing this variability. But this is information relating to the DV, so including it as a feature doesn't make sense. I'll have to think about that.  

I'll dig into formatting the hours first.



## Hours of Operation

One straight-forward task to start with is converting the hours for each day into new vars that just have the total amount of hours open on that day. Later on I'll want to consider how to capture the information about how early / late those hours are.  

```{r}
print(typeof(yelp$mon))
print(class(yelp$mon))
s1 <- yelp$mon[1]
s2 <- yelp$tue[1]
print(c(s1,s2))
```

Alright so, for all of these, I'll want to keep only the data before the first `\n`, then I'll want to split the range into two vars for the beginning and the end.



```{r}
before <- '^\\d+?:\\d+\\s\\w\\w'
after <- '(\\d+:\\d+\\s\\w\\w)(\\\n.*)?$'
simpleTime <- '\\d+:\\d\\d\\s\\w\\w'
```

```{r}

test <- yelp[match(unique(yelp$name), yelp$name),]
test <- test %>% 
  select(name, mon, tue, wed, thu, fri, sat, sun) %>% 
  gather(day, times, mon:sun) %>% 
  separate(times, into=c('open', 'close'), sep='-') %>% 
  mutate(open = str_extract(open, simpleTime),
         close = str_extract(close, simpleTime))# %>% 



test$open <- strptime(test$open, format = '%I:%M %p')
test$close <- strptime(test$close, format = '%I:%M %p')
test$open <- times(sub('.*\\s+', '', test$open))
test$close <- times(sub('.*\\s+', '', test$close))
test$totalHours <- as.numeric(abs(test$close - test$open))


yelp <- test %>% 
  mutate(totalHours = abs(test$close - test$open)) %>% 
  gather(timePeriod, time, open:close) %>% 
  gather(sumOrTime, value, totalHours, time) %>% 
  unite(variable, day, timePeriod, sumOrTime) %>% 
  spread(variable, value) %>% 
  inner_join(yelp)
  
## open and close for total hours are the same thing
sum(yelp$thu_open_totalHours == yelp$thu_close_totalHours, na.rm = TRUE) == nrow(yelp[!is.na(yelp$thu_open_totalHours),])

yelp <- yelp %>% 
  select(-contains('_close_totalhours'))

```
Whew, that was tough.


## Price Range

I'm going to get together a function for sorting the various levels of `price_range` into bins to be used as an ordinal factor. 

```{r}
print(summary(factor(yelp$price_range)))
```

I think the general idea will be to make three levels: `['cheap', 'moderate', 'expensive']` where the cutoffs are as follows:  

* `< $10 = cheap`  
* `>= $10 & < $30 = moderate`  
* `> $30 = expensive`

```{r}
cleanPriceRange <- function(price_range){
  ### takes as input char vector price_range
  ### returns price range (still as a char var) sorted into three groups
  
  cheapBucket <- c('under $10', 'inexpensive')
  moderateBucket <- c('$11-30', 'A$16-35', 'Moderate')
  expensiveBucket <- c('$31-60')
  
  price_range <- tolower(price_range)
  
  ## regrettably (bc we need vectorized operations) there's no great way around the disgusting ifelse() chain
  out <- ifelse(price_range %in% cheapBucket, 'cheap',
                ifelse(price_range %in% moderateBucket, 'moderate',
                       ifelse(price_range %in% expensiveBucket, 'expensive', NA)))
  
  return(out)
}
```

```{r}
yelp <- yelp %>% 
  mutate(price_range_messy = price_range) %>% 
  select(-price_range) %>% 
  mutate(price_range = cleanPriceRange(price_range_messy))

print(summary(factor(yelp$price_range)))
yelp[500:550,]
```


```{r}
yelp <- yelp %>% 
  select(-price_range_messy)
```

Alright that feature should be mostly good to go. Might want to dummy code it out later, but for now it's fine.


## More Info
Time to deal with the nuanced info vars. The mission will be to try and convert them to long.. but we'll see how it goes.  

First we'll want to knock out all the NAs. These are breweries that were unable to be scraped. Then I'll need to get a sense of the degree of variablity in the `moreinfovar` variable.

```{r}
yelp <- yelp %>% 
  filter(!is.na(moreinfovar))
```

```{r}
summary(factor(yelp$moreinfovar))
```

Hmm, yea this is tough. Some of these (e.g., `Has Soy-Free Options` & `Has Keto Options`) are only represented very few times. Trying to convert these to wide will result in NAs pretty much across the board. I do want to avoid pruning these, as there is much more data yet to be collected. I might just try to convert to wide, then implement a NA quality threshold before moving on to analysis. The basic idea will be to drop vars with too many NAs, and tackle imputing values for those that seem worth saving.  

Here we go:

```{r}
## removing all breweries that have duplicate entries for moreinfo var (80 total)
badBrews <- yelp %>% 
  group_by(name) %>% 
  summarize(dups = sum(duplicated(moreinfovar))) %>% 
  filter(dups >= 1) %>% 
  select(name)

yelp <- yelp[!(yelp$name %in% badBrews$name),]

yelp <- yelp %>% 
  spread(moreinfovar, moreinfoval) 

colnames(yelp) <- tolower(colnames(yelp))

head(yelp)
```
 
This seems surprisingly alright. Luckily, most of these features seem to be coded as `Yes` / `No` -- I'll have to think about how to handle the ones that aren't (e.g., `Parking`)  

The following function computes the proportion of missing data (from most to least) for all variables:

*realizing i could've done this with `sapply()`, but this formats it nicely*


```{r}
summarizeNAs <- function(data) {
  holder <- data.frame(colName = character(), propMissing = numeric(), stringsAsFactors = FALSE)
  count <- 0
  for (colIndex in 1:(ncol(data))) {
    count <- count+ 1
    holder[count, 1] <- colnames(data)[colIndex]
    holder[count, 2] <- nrow(data[is.na(data[,colIndex]),]) / nrow(data)
  }
  return(holder[order(holder$propMissing, decreasing = TRUE),])
}

naSummary <- summarizeNAs(yelp)
summarizeNAs(yelp)
```

Unfortunately, as of now, there are only very few of these special info variables (5) that have < 20% of values missing. For now, I'm going to drop all vars > 50% missing values.


```{r}
yelp <- yelp[, !(colnames(yelp) %in% naSummary[naSummary$propMissing > .5, 1])]
```

This leaves the following vars to work with:  

```{r}
colnames(yelp)
```


Writing a quick mode-imputation function:

```{r}
getmode <- function(v) {
   v <- na.omit(v)
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}


modeImputation <- function(data){
  for (column in colnames(data)) {
    #print(column)
    modeValue <- getmode(data[,column])
    data[is.na(data[,column]), column] <- modeValue #rep(modeValue, length(data[is.na(data$column), column]))
  }
  return(data)
}

yelp <- modeImputation(yelp)
summarizeNAs(yelp)
```

Very good.  

Quickly dealing with the parking variable:  

```{r}
yelp <- yelp %>% 
  mutate(parking = ifelse(grepl(',', parking), 'multiple', parking))
```


Let's convert some of these to dummy variables:

```{r}
colnames(yelp) <- gsub('-', '_', colnames(yelp))
colnames(yelp) <- tolower(gsub(' ', '_', colnames(yelp)))

dummyCols <- yelp %>% 
  ## im gonna leave the category var alone for now 
  select(price_range:wheelchair_accessible)

yelp <- dummy_cols(yelp, select_columns = colnames(dummyCols), remove_first_dummy = TRUE)

colnames(yelp) <- tolower(colnames(yelp))

yelp <- yelp %>% 
  mutate(accepts_credit_cards_yes = ifelse(accepts_credit_cards_no == 0, 1, 0),
         bike_parking_yes = ifelse(bike_parking_no == 1, 0, 1),
         wheelchair_accessible_yes = ifelse(wheelchair_accessible_no == 1, 0, 1),
         accepts_apple_pay_yes = ifelse(accepts_apple_pay_no == 1, 0, 1)) %>% 
  select(-bike_parking_no, -accepts_credit_cards_no, -claimed_status, -accepts_apple_pay_no, -wheelchair_accessible_no, -(fri:wed))
```

### Deal with Reviews




```{r}
yelp <- yelp %>% 
  mutate(reviews = as.numeric(str_extract(reviews, '\\d+')),
        my_ratings = (x1_star_count * 1 + x2_star_count * 2 + x3_star_count * 3 + x4_star_count * 4 + x5_star_count * 5) / sum(c(x1_star_count,x2_star_count,x3_star_count,x4_star_count,x5_star_count)))
```


```{r}
store <- numeric()
for (row in 1:(nrow(yelp))) {
  store[row] <- with(yelp[row,], (x1_star_count * 1 + x2_star_count * 2 + x3_star_count * 3 + x4_star_count * 4 + x5_star_count * 5) / sum(c(x1_star_count,x2_star_count,x3_star_count,x4_star_count,x5_star_count)))
}

yelp$my_ratings <- store
```

Calculating the weighted scores by hand allows for more variablility.

I think that pretty much wraps it up.

```{r}
head(yelp)
```

### Merging with some of the original data


```{r}
breweries <- read.csv('../../../data/breweries.csv')
census <- read.csv('../../../data/censusData.csv')
colnames(census) <- tolower(colnames(census))
census <- census[!(census$name %in% c('United States', 'Puerto Rico Commonwealth', 'District of Columbia')),]
census <- census %>% 
  select(name, popestimate2017) %>% 
  rename(state = name, population = popestimate2017)

yelp <- breweries %>% 
  select(name, state) %>% 
  inner_join(yelp, by = 'name')

breweries <- breweries %>% 
  inner_join(census) %>% 
  select(name, state, population) %>% 
  group_by(state) %>% 
  summarize(population = max(population), nBreweries = n()) %>% 
  mutate(breweriesToPpl = nBreweries / population)

yelp <- yelp %>% 
  inner_join(breweries, by = 'state')

```


```{r}
write.csv(yelp, '../data/cleanYelp.csv', row.names = FALSE)
```



