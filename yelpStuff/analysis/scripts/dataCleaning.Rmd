---
title: "Yelp Data Cleaning"
author: "Dave Braun"
date: "February 19, 2019"
output:
  html_document:
    df_print: paged
---


This document is intended for the cleaning of data scraped from yelp. 

## The Data

The general layout of the data is as follows:  


```{r include = FALSE}
library(knitr)
library(tidyverse)
library(data.table)
library(chron)
library(imputeMissings)
library(fastDummies)
library(stringr)
```


```{r}
yelp <- read.csv('../data/yelpData.csv', stringsAsFactors = FALSE)
colnames(yelp) <- tolower(colnames(yelp))
## drop breweries that couldn't be scraped
yelp <-yelp[!is.na(yelp$ratings),]
yelp
```

```{r}
yelp %>% 
  group_by(name) %>% 
  summarize(n()) %>% 
  nrow()
```

*Of the nearly 8,000 breweries on [open brewery db](https://openbrewerydb.org), I was able to successfully scrape 2465 of them on the first pass.*

## Important Variables

Off the bat, I was a little worried because `rating` appears null across the board, but I see that it's redundant with `ratings`, which is present.  Some notes about specific vars:

* `moreInfoVar` The trickiest part will be handling `moreInfoVar`, as this capures idiosyncratic additional business detail, the features of which vary across businesses. It's represented in long form for now, but I'll want to experiment with getting it in wide so each brewery can be represented as a single observation.  

* `Hours` I'll probably want to parse the hours into total number of hours open each day. But this doesn't capture whether a brewery is open earlier / later. Maybe try to code for that with a different variable. Watch out for the `Open now` that's stored in the `Mon` hours. This will show up in the `Tues` hours in the data I'm collecting today.

* `claimed_status` Dunno what this is, seems to be constant across breweries so might as well drop.  

* `health_rating` I really like the idea of this one, but based on the first 400 breweries, this var is very sparse and heterogeneous.  

* `price_range` This one is important. Looks like it's represented with either an adjective (e.g., `moderate`) or price range (e.g., `$11-30`).  

* `ratings` `reviews` obv the dependent vars. I'll have to think about exactly how to negotiate the fact that there's two of them.  

* `ratings_histogram` I like the idea of capturing this variability. But this is information relating to the DV, so including it as a feature doesn't make sense. I'll have to think about that.  

I'll dig into formatting the hours first.



## Hours of Operation

One straight-forward task to start with is converting the hours for each day into new vars that just have the total amount of hours open on that day. Later on I'll want to consider how to capture the information about how early / late those hours are.  

```{r}
print(typeof(yelp$mon))
print(class(yelp$mon))
s1 <- yelp$mon[1]
s2 <- yelp$tue[1]
print(c(s1,s2))
```

Alright so, for all of these, I'll want to keep only the data before the first `\n`, then I'll want to split the range into two vars for the beginning and the end.

Then convert to time, compute the total hours that a brewery is open, reshape it all so that all hours-of-operation information for each brewery is on one row, and join it back with the main data.

```{r}
## Regex to extract time
simpleTime <- '\\d+:\\d\\d\\s\\w\\w'

## keep one row per brewery
test <- yelp[match(unique(yelp$name), yelp$name),]

## Regex and time conversion to get time data to behave like time data

test <- test %>% 
  select(name, mon, tue, wed, thu, fri, sat, sun) %>% 
  gather(day, times, mon:sun) %>% 
  ## Separate into open and close times based on the '-'
  separate(times, into=c('open', 'close'), sep='-') %>% 
  ## match only strings matching time format
  mutate(open = str_extract(open, simpleTime),
         close = str_extract(close, simpleTime))# %>% 

## Convert to time
test$open <- strptime(test$open, format = '%I:%M %p')
test$close <- strptime(test$close, format = '%I:%M %p')
## Strip out the date, keep only the time
test$open <- times(sub('.*\\s+', '', test$open))
test$close <- times(sub('.*\\s+', '', test$close))

## Doing some serious data reshaping to get time information on one line per brewery

## Goal is to spread time open, time closed, and total time to wide vars of the form: day_open/close_TotalTime/Not
yelp <- x <- test %>% 
  ## Compute total hours open
  mutate(totalHours = abs(test$close - test$open)) %>% 
  ## Gather open / close times
  gather(timePeriod, time, open:close) %>% 
  ## Gather total hours and open / close times
  gather(TotalOrTime, value, totalHours, time) %>% 
  ## Combine day, open / close times, total time
  unite(variable, day, timePeriod, TotalOrTime) %>% 
  ## Spread across the united variable
  spread(variable, value) %>% 
  inner_join(yelp)
  
## open and close for total hours are the same thing
sum(yelp$thu_open_totalHours == yelp$thu_close_totalHours, na.rm = TRUE) == nrow(yelp[!is.na(yelp$thu_open_totalHours),])

yelp <- yelp %>% 
  select(-contains('_close_totalhours'))

yelp

```
Whew, that was tough.


## Price Range

I'm going to get together a function for sorting the various levels of `price_range` into bins to be used as an ordinal factor. 

```{r}
print(summary(factor(yelp$price_range)))
```

I think the general idea will be to make three levels: `['cheap', 'moderate', 'expensive']` where the cutoffs are as follows:  

* `< $10 = cheap`  
* `>= $10 & < $30 = moderate`  
* `> $30 = expensive`

```{r}
cleanPriceRange <- function(price_range){
  ### takes as input char vector price_range
  ### returns price range (still as a char var) sorted into three groups
  
  cheapBucket <- c('under $10', 'inexpensive')
  moderateBucket <- c('$11-30', 'A$16-35', 'Moderate')
  expensiveBucket <- c('$31-60')
  
  price_range <- tolower(price_range)
  
  ## regrettably (bc we need vectorized operations) there's no great way around the disgusting ifelse() chain
  out <- ifelse(price_range %in% cheapBucket, 'cheap',
                ifelse(price_range %in% moderateBucket, 'moderate',
                       ifelse(price_range %in% expensiveBucket, 'expensive', NA)))
  
  return(out)
}
```

```{r}
yelp <- yelp %>% 
  mutate(price_range_messy = price_range) %>% 
  select(-price_range) %>% 
  mutate(price_range = cleanPriceRange(price_range_messy))

print(summary(factor(yelp$price_range)))
yelp[500:550,]
```


```{r}
yelp <- yelp %>% 
  select(-price_range_messy)
```

Alright that feature should be mostly good to go. Might want to dummy code it out later, but for now it's fine.


## More Info
Time to deal with the nuanced info vars. The mission will be to try and convert them to wide.. but we'll see how it goes.  

I'll need to get a sense of the degree of variablity in the `moreinfovar` variable.


```{r}
summary(factor(yelp$moreinfovar))
```

Hmm, yea this is tough. Some of these (e.g., `Has Soy-Free Options` & `Has Keto Options`) are only represented very few times. Trying to convert these to wide will result in NAs pretty much across the board. I do want to avoid pruning these, as there is much more data yet to be collected. I might just try to convert to wide, then implement a NA quality threshold before moving on to analysis. The basic idea will be to drop vars with too many NAs, and tackle imputing values for those that seem worth saving.  

Here we go:

```{r}
## Some breweries have redundant entries for moreinfovar (80 total), need to drop these for spread to work 
badBrews <- yelp %>% 
  group_by(name) %>% 
  summarize(dups = sum(duplicated(moreinfovar))) %>% 
  filter(dups >= 1) %>% 
  select(name)

yelp <- yelp[!(yelp$name %in% badBrews$name),]

yelp <- yelp %>% 
  spread(moreinfovar, moreinfoval) 

colnames(yelp) <- tolower(colnames(yelp))

yelp
```
 
This seems surprisingly alright. Luckily, most of these features seem to be coded as `Yes` / `No` -- I'll have to think about how to handle the ones that aren't (e.g., `Parking`)  

The following function computes the proportion of missing data (from most to least) for all variables:

*realizing i could've done this with `sapply()`, but this formats it nicely*


```{r}
summarizeNAs <- function(data) {
  ## Returns a sorted df where column one is the name of the original column and column two is the proportion of data missing from that column
  
  holder <- data.frame(colName = character(), propMissing = numeric(), stringsAsFactors = FALSE)
  count <- 0
  for (colIndex in 1:(ncol(data))) {
    count <- count+ 1
    holder[count, 1] <- colnames(data)[colIndex]
    holder[count, 2] <- nrow(data[is.na(data[,colIndex]),]) / nrow(data)
  }
  return(holder[order(holder$propMissing, decreasing = TRUE),])
}

naSummary <- summarizeNAs(yelp)
summarizeNAs(yelp)
```

Unfortunately, as of now, there are only very few of these special info variables (5) that have < 20% of values missing. For now, I'm going to drop all vars > 50% missing values.


```{r}
yelp <- yelp[, !(colnames(yelp) %in% naSummary[naSummary$propMissing > .5, 1])]
```

This leaves the following vars to work with:  

```{r}
colnames(yelp)
```


Writing a quick mode-imputation function:

```{r}
getmode <- function(v) {
   v <- na.omit(v)
   uniqv <- unique(v)
   ## tabulate takes the integer-valued vector bin and counts the number of times each integer occurs in it.
   uniqv[which.max(tabulate(match(v, uniqv)))]
}


modeImputation <- function(data){
  for (column in colnames(data)) {
    #print(column)
    modeValue <- getmode(data[,column])
    data[is.na(data[,column]), column] <- modeValue #rep(modeValue, length(data[is.na(data$column), column]))
  }
  return(data)
}

yelp <- modeImputation(yelp)
summarizeNAs(yelp)
```

Very good.  

Quickly dealing with the parking variable:  

```{r}
yelp <- yelp %>% 
  ## If there's more than one parking option just label it 'multiple'
  mutate(parking = ifelse(grepl(',', parking), 'multiple', parking))
```


Let's convert some of these to dummy variables:

```{r}
colnames(yelp) <- gsub('-', '_', colnames(yelp))
colnames(yelp) <- tolower(gsub(' ', '_', colnames(yelp)))

dummyCols <- yelp %>% 
  ## im gonna leave the category var alone for now 
  select(price_range:wheelchair_accessible)

yelp <- dummy_cols(yelp, select_columns = colnames(dummyCols), remove_first_dummy = TRUE)

colnames(yelp) <- tolower(colnames(yelp))

## Converting so dummy vars reflect yes value rather than no
yelp <- yelp %>% 
  mutate(accepts_credit_cards_yes = ifelse(accepts_credit_cards_no == 0, 1, 0),
         bike_parking_yes = ifelse(bike_parking_no == 1, 0, 1),
         wheelchair_accessible_yes = ifelse(wheelchair_accessible_no == 1, 0, 1),
         accepts_apple_pay_yes = ifelse(accepts_apple_pay_no == 1, 0, 1)) %>% 
  select(-bike_parking_no, -accepts_credit_cards_no, -claimed_status, -accepts_apple_pay_no, -wheelchair_accessible_no, -(fri:wed))
```

### Deal with Reviews

```{r}
yelp <- yelp %>% 
  mutate(reviews = as.numeric(str_extract(reviews, '\\d+')))#,
        #my_ratings = (x1_star_count * 1 + x2_star_count * 2 + x3_star_count * 3 + x4_star_count * 4 + x5_star_count * 5) / sum(c(x1_star_count,x2_star_count,x3_star_count,x4_star_count,x5_star_count)))
```


```{r}
## Had to resort to a for loop to calculate weighted ratings 
store <- numeric()
for (row in 1:(nrow(yelp))) {
  store[row] <- with(yelp[row,], (x1_star_count * 1 + x2_star_count * 2 + x3_star_count * 3 + x4_star_count * 4 + x5_star_count * 5) / sum(c(x1_star_count,x2_star_count,x3_star_count,x4_star_count,x5_star_count)))
}

yelp$my_ratings <- store

yelp %>% 
  gather(rating_type, rating, ratings, my_ratings) %>% 
  mutate(rating_type = fct_recode(rating_type, 'Yelp Ratings' = 'ratings', 'My Converted Ratings' = 'my_ratings')) %>% 
  ggplot(aes(x = rating)) + geom_density(fill = 'blue', alpha = .8) + xlab('Ratings') + facet_wrap(~rating_type) + 
  theme_bw() +
  theme(strip.background = element_rect(fill = 'white', color = 'black')) 
  
  
  
```

Calculating the weighted scores by hand allows for more variablility.

I think that pretty much wraps it up.

```{r}
yelp
```

### Merging with some of the original data


```{r}
breweries <- read.csv('../../../data/breweries.csv')
census <- read.csv('../../../data/censusData.csv')
colnames(census) <- tolower(colnames(census))
census <- census[!(census$name %in% c('United States', 'Puerto Rico Commonwealth', 'District of Columbia')),]
census <- census %>% 
  select(name, popestimate2017) %>% 
  rename(state = name, population = popestimate2017)

yelp <- breweries %>% 
  select(name, state) %>% 
  inner_join(yelp, by = 'name')

breweries <- breweries %>% 
  inner_join(census) %>% 
  select(name, state, population) %>% 
  group_by(state) %>% 
  summarize(population = max(population), nBreweries = n()) %>% 
  mutate(breweriesToPpl = nBreweries / population)

yelp <- yelp %>% 
  inner_join(breweries, by = 'state')

```


```{r}
write.csv(yelp, '../data/cleanYelp.csv', row.names = FALSE)
```



