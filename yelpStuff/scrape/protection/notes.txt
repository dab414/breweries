very good overview: https://blog.hartleybrody.com/web-scraping-proxies/

common ways sites detect crawlers:

	repetitive IPs
	repetitive browser headers
	the risk of detection with these two ^ is affected by the latency of requests with any set of IPs or headers
	honey pots
		dosen't seem too concerning

ip rotation

	ipconfig.me/ip
		returns just text of current ip

	this part of the process has proven to be complicated, and it's probably the most important

	my one thought was to try and leverage PIA to rotate IPs
		this has proven very difficult
		python can most easily work with proxies
		PIA offers a proxy, but it seems like there's only one IP that i can get from that proxy
		trying to manage the actual vpn from python seems ideal, but this also seems very difficult
			very sparse support from managing vpn from command line (openVPN, via bash)
			little to nothing about how to extend that to be under the control of a python script

		i emailed pia support to see if there's any way to rotate IPs with the sock5 proxy, so we'll see

	maybe the easier way to approach it is to use these public, free SSL proxy services
		https://sslproxies.org/
		guide: https://codelike.pro/create-a-crawler-with-rotating-ip-proxy-in-python/

		'proxies' argument in requests.get():
			dict where {'protocol': 'ip:port'}
			protocol is one of ['http', 'https', 'ftp']

header rotation
	module: fake_useragent
	from fake_useragent import UserAgent
	ua = UserAgent()
	ua.random

general protection logic
	
	so i think keep headers / ip the same for a given search / click sequence
		with some randomly generated delay inbetween
		N(1,0.5)

	then randomly generate headers and sequentially ip between requests
		no need for a delay between accessing specific page and next search
		i think it's better to systematically go through a list of ips rather than randomly

	need some mechanism for detecting ips that have been blacklisted
		special events need to occur every time a request results in a null set
		the information for that brewery should be added to some list
			the logic being that, if the null set is due to a blacklisted ip, running the request on that brewery again might be successful
		each time an ip returns a null set, some information should be added to that ips 'profile'
			maybe a running dict with IPs as keys and for values counts of how many times the ip has produced a null set
			once IP reaches a threshold, it's removed

	incorporate random requests

	it'll be important to only use ips from first world countries i think
		i randomly tried with not a first world country and the request was blocked
